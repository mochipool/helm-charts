# Mainnet Relay Node - Production Ready Example
# This is a complete, production-ready relay configuration for Cardano Mainnet
# Optimized for high availability and performance
# Socket can be shared by created PVC

# ===========================================
# BASIC CONFIGURATION
# ===========================================
replicaCount: 2  # Multiple replicas for HA

# ===========================================
# CARDANO NODE CONFIGURATION
# ===========================================
cardanoNode:
  network: "mainnet"
  magic: "764824073"
  
  # Relay node configuration
  blockProducer: false
  
  # Enable Mithril for fast sync
  mithril:
    enabled: true
    restoreSnapshot: true
    aggregatorEndpoint: "https://aggregator.release-mainnet.api.mithril.network/aggregator"
    genesisVerificationKey: "5b3133312c36362c3134302c3138352c3133382c31312c3233372c3230372c3235302c3134342c32372c322c3138382c33302c31322c38312c3135352c3230342c31302c3137392c37352c32332c3133382c3139362c3231372c352c31342c32302c35372c37392c33392c3137365d"
    ancillaryVerificationKey: "5b32332c37312c39362c3133332c34372c3235332c3232362c3133362c3233352c35372c3136342c3130362c3138362c322c32312c32392c3132302c3136332c38392c3132312c3137372c3133382c3230382c3133382c3231342c39392c35382c32322c302c35382c332c36395d"

  # Override requirements for mainnet
  config:
    requiresNetworkMagic: "RequiresNoMagic"

  # Mainnet network topology
  topology:
    bootstrapPeers:
      - address: "backbone.cardano.iog.io"
        port: 3001
      - address: "backbone.mainnet.emurgornd.com"
        port: 3001
      - address: "backbone.mainnet.cardanofoundation.org"
        port: 3001
    localRoots:
      - accessPoints:
          - address: "cardano-bp-cardano-node-p2p.cardano-mainnet.svc.cluster.local"  # Block Producer Internal DNS Name
            port: 3001
            description: "block-producer-1"
        advertise: false
        trustable: true
        hotValency: 2
    publicRoots:
      - accessPoints: []
        advertise: false
    useLedgerAfterSlot: 128908821

# ===========================================
# FORGE MANAGER V2.0
# ===========================================
forgeManager:
  enabled: false
  
# ===========================================
# STORAGE CONFIGURATION
# ===========================================
persistence:
  ledger:
    enabled: true
    size: 400Gi  # Mainnet requires substantial storage
    # storageClass: "fast-ssd"  # Use high-performance storage class
  socket:
    enabled: true

# ===========================================
# RESOURCE CONFIGURATION (Production)
# ===========================================
resources:
  cardanoNode:
    limits:
      cpu: 4000m
      memory: 16Gi
    requests:
      cpu: 2000m
      memory: 8Gi
  
  forgeManager:
    limits:
      cpu: 200m
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 256Mi

# ===========================================
# NETWORKING
# ===========================================
service:
  p2p:
    type: LoadBalancer  # External access for relay
    port: 3001
    annotations:
      # Example for AWS NLB
      service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
      service.beta.kubernetes.io/aws-load-balancer-scheme: "internet-facing"

# ===========================================
# MONITORING
# ===========================================
monitoring:
  enabled: true
  serviceMonitor:
    enabled: true  # Enable for production monitoring
    interval: 30s
    scrapeTimeout: 10s

# ===========================================
# POD CONFIGURATION
# ===========================================
podLabels:
  network: "mainnet"
  environment: "production"
  node-type: "relay"

podAnnotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "12798"
  prometheus.io/path: "/metrics"

# ===========================================
# HIGH AVAILABILITY
# ===========================================
affinity:
  podAntiAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
            - key: app.kubernetes.io/name
              operator: In
              values:
                - cardano-node
            - key: network
              operator: In
              values:
                - mainnet
            - key: node-type
              operator: In
              values:
                - relay
        topologyKey: kubernetes.io/hostname

# ===========================================
# POD DISRUPTION BUDGET
# ===========================================
podDisruptionBudget:
  enabled: true
  minAvailable: 1  # Always keep at least 1 replica running

# ===========================================
# SECURITY CONTEXT
# ===========================================
securityContext:
  runAsNonRoot: true
  runAsUser: 10001
  runAsGroup: 10001
  allowPrivilegeEscalation: false
  readOnlyRootFilesystem: false

podSecurityContext:
  fsGroup: 10001
  runAsNonRoot: true
  runAsUser: 10001
  fsGroupChangePolicy: Always

# ===========================================
# NETWORK POLICY (Production Security)
# ===========================================
networkPolicy:
  enabled: true
  ingress:
    # Allow P2P from anywhere
    p2pFromCIDR: "0.0.0.0/0"
    # Allow metrics from monitoring namespace
    metricsFromNamespaces:
      - monitoring
  egress:
    # Allow P2P to anywhere
    p2pToCIDR: "0.0.0.0/0"
    # Allow HTTPS for genesis files and Mithril
    httpsToCIDR: "0.0.0.0/0"

# ===========================================
# DEPLOYMENT INSTRUCTIONS
# ===========================================
# 
# This example provides a complete mainnet relay deployment.
# Follow these steps to deploy:
#
# 1. Create namespace:
#    kubectl create namespace cardano-mainnet
#
# 2. Review and customize resource allocation:
#    - Ensure your cluster has sufficient CPU/memory
#    - Consider using dedicated nodes with high-performance storage
#
# 3. Deploy the relay:
#    helm install cardano-relay ./charts/cardano-node \
#      --namespace cardano-mainnet \
#      --values ./charts/cardano-node/examples/mainnet-relay.yaml
#
# 4. Monitor the deployment:
#    kubectl logs -f cardano-relay-0 -c cardano-node -n cardano-mainnet
#    # You should see Mithril snapshot restoration messages
#
# 5. Get external IP for connecting block producers:
#    kubectl get svc cardano-relay-cardano-node-p2p -n cardano-mainnet
#
# ===========================================
# VERIFICATION STEPS
# ===========================================
#
# After deployment, verify everything is working:
#
# 1. Check node sync status:
#    kubectl exec -it cardano-relay-0 -c cardano-node -n cardano-mainnet -- \
#      cardano-cli query tip --mainnet --socket-path /ipc/node.socket
#
# 2. Check that the node socket exists:
#    kubectl exec -it cardano-relay-0 -c cardano-node -n cardano-mainnet -- \
#      ls -la /ipc/node.socket
#
# 3. Test network connectivity:
#    kubectl exec -it cardano-relay-0 -c cardano-node -n cardano-mainnet -- \
#      nc -zv backbone.cardano.iog.io 3001
#
# 4. Monitor resource usage:
#    kubectl top pods -n cardano-mainnet
#
# 5. Check high availability:
#    kubectl get pods -n cardano-mainnet -o wide
#    # Verify pods are running on different nodes
#
# ===========================================
# CUSTOMIZATION
# ===========================================
#
# Common customizations for production:
#
# 1. Resource allocation:
#    - Scale CPU/memory based on your infrastructure
#    - Use node selectors for dedicated hardware
#
# 2. Storage optimization:
#    - Use high-performance storage classes (SSD, NVMe)
#    - Consider larger storage sizes for future growth
#
# 3. Networking:
#    - Configure LoadBalancer annotations for your cloud provider
#    - Set up DNS names for your relay endpoints
#
# 4. Monitoring:
#    - Enable ServiceMonitor for Prometheus
#    - Set up alerting for node health and sync status
#
# 5. Security:
#    - Enable NetworkPolicy for production security
#    - Consider using Pod Security Standards
#
# ===========================================
# EXPECTED BEHAVIOR
# ===========================================
#
# 1. Two relay pods will start with anti-affinity (different nodes)
# 2. Node will restore from Mithril snapshot (faster than full sync)
# 3. External LoadBalancer service will provide P2P access
# 4. Pods will maintain high availability with PDB
# 5. Metrics will be available for monitoring
#
# ===========================================
# PRODUCTION CONSIDERATIONS
# ===========================================
#
# 1. Node placement:
#    - Use dedicated nodes with high-performance storage
#    - Consider different availability zones
#
# 2. Monitoring:
#    - Set up comprehensive monitoring and alerting
#    - Monitor sync status, peer connections, and resource usage
#
# 3. Backup strategy:
#    - Regular snapshots of persistent volumes
#    - Consider database backups for quick recovery
#
# 4. Security:
#    - Regular security updates for node images
#    - Network segmentation and firewall rules
#
# 5. Performance tuning:
#    - Adjust resources based on actual usage patterns
#    - Consider SSD storage for optimal performance
#
# ===========================================
# TROUBLESHOOTING
# ===========================================
#
# Common production issues:
#
# 1. Pod stuck in pending:
#    - Check resource availability and node capacity
#    - Verify storage class exists and has capacity
#
# 2. Slow sync performance:
#    - Check storage performance (IOPS, throughput)
#    - Monitor network connectivity to bootstrap peers
#
# 3. High resource usage:
#    - Monitor and adjust resource limits
#    - Consider scaling to dedicated nodes
#
# 4. LoadBalancer issues:
#    - Verify cloud provider LoadBalancer configuration
#    - Check security groups and firewall rules
